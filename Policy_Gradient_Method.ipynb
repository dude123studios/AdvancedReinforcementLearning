{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Policy Gradient Reward-to-go",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPXitA38tOF1/umMeBqlLQS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dude123studios/AdvancedReinforcementLearning/blob/main/Policy_Gradient_Method.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ublvA9qL3szm"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "import gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4n4eCriw34sw"
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "state_shape = env.observation_space.shape[0]\n",
        "action_shape = env.action_space.n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EAF_vNG4Kk0"
      },
      "source": [
        "gamma = 0.95\n",
        "\n",
        "def discount_and_normalize_rewards(episode_rewards):\n",
        "\n",
        "    discounted_rewards = np.zeros_like(episode_rewards)\n",
        "\n",
        "    reward_to_go = 0.0\n",
        "\n",
        "    for i in reversed(range(len(episode_rewards))):\n",
        "        reward_to_go = reward_to_go * gamma + episode_rewards[i]\n",
        "        discounted_rewards[i] = reward_to_go\n",
        "    \n",
        "    discounted_rewards -= np.mean(discounted_rewards)\n",
        "    discounted_rewards /= np.std(discounted_rewards)\n",
        "\n",
        "    return discounted_rewards"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHnTKR_rDyXJ"
      },
      "source": [
        "state_ph = tf.placeholder(tf.float32, [None, state_shape], name='state_ph')\n",
        "action_ph = tf.placeholder(tf.int32, [None, action_shape], name='action_ph')\n",
        "discounted_rewards_ph = tf.placeholder(tf.float32, [None,], name='discounted_rewards')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3FW-heEEf4i"
      },
      "source": [
        "layer1 = tf.layers.dense(state_ph, units=32, activation=tf.nn.relu)\n",
        "layer2 = tf.layers.dense(layer1, units=action_shape)\n",
        "prob_dist = tf.nn.softmax(layer2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYGsTvrXFMH9"
      },
      "source": [
        "#Calculates -(0*log(pi(a1|s)) + 0*log(pi(a2|s)) + 1*log(pi(action taken|s) + ...))\n",
        "#This means it really just gets the log of the probability of the action taken \n",
        "neg_loss_policy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=layer2, labels = action_ph)\n",
        "\n",
        "#Policy gradient\n",
        "loss = tf.reduce_mean(neg_loss_policy * discounted_rewards_ph)\n",
        "\n",
        "train = tf.train.AdamOptimizer(1e-2).minimize(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRXg6xmHLWAM",
        "outputId": "619f2c8e-be11-49d3-b2b2-d6b18ed06167"
      },
      "source": [
        "num_iterations = 1000\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "\n",
        "        episode_states, episode_actions, episode_rewards = [], [], []\n",
        "\n",
        "        done = False\n",
        "\n",
        "        Return = 0\n",
        "\n",
        "        state = env.reset()\n",
        "\n",
        "        while not done:\n",
        "\n",
        "            state = state.reshape([1, 4])\n",
        "\n",
        "            pi = sess.run(prob_dist, feed_dict={state_ph: state})\n",
        "\n",
        "            a = np.random.choice(range(pi.shape[1]), p = pi.ravel())\n",
        "\n",
        "            next_state, reward, done, info = env.step(a)\n",
        "\n",
        "            #env.render()\n",
        "\n",
        "            Return += reward\n",
        "\n",
        "            action = np.zeros(action_shape)\n",
        "            action[a] = 1\n",
        "\n",
        "            episode_states.append(state)\n",
        "            episode_actions.append(action)\n",
        "            episode_rewards.append(reward)\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        discounted_rewards = discount_and_normalize_rewards(episode_rewards)\n",
        "\n",
        "        feed_dict = {state_ph: np.vstack(np.array(episode_states)), \n",
        "                     action_ph: np.vstack(np.array(episode_actions)), \n",
        "                     discounted_rewards_ph: discounted_rewards}\n",
        "        \n",
        "        loss_, _ = sess.run([loss, train], feed_dict=feed_dict)\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print('Iteration: {}, Return: {}'.format(i, Return))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 0, Return: 55.0\n",
            "Iteration: 10, Return: 22.0\n",
            "Iteration: 20, Return: 47.0\n",
            "Iteration: 30, Return: 48.0\n",
            "Iteration: 40, Return: 31.0\n",
            "Iteration: 50, Return: 28.0\n",
            "Iteration: 60, Return: 135.0\n",
            "Iteration: 70, Return: 158.0\n",
            "Iteration: 80, Return: 165.0\n",
            "Iteration: 90, Return: 200.0\n",
            "Iteration: 100, Return: 200.0\n",
            "Iteration: 110, Return: 181.0\n",
            "Iteration: 120, Return: 129.0\n",
            "Iteration: 130, Return: 54.0\n",
            "Iteration: 140, Return: 200.0\n",
            "Iteration: 150, Return: 200.0\n",
            "Iteration: 160, Return: 137.0\n",
            "Iteration: 170, Return: 163.0\n",
            "Iteration: 180, Return: 197.0\n",
            "Iteration: 190, Return: 163.0\n",
            "Iteration: 200, Return: 200.0\n",
            "Iteration: 210, Return: 200.0\n",
            "Iteration: 220, Return: 157.0\n",
            "Iteration: 230, Return: 200.0\n",
            "Iteration: 240, Return: 200.0\n",
            "Iteration: 250, Return: 200.0\n",
            "Iteration: 260, Return: 200.0\n",
            "Iteration: 270, Return: 200.0\n",
            "Iteration: 280, Return: 200.0\n",
            "Iteration: 290, Return: 200.0\n",
            "Iteration: 300, Return: 200.0\n",
            "Iteration: 310, Return: 200.0\n",
            "Iteration: 320, Return: 200.0\n",
            "Iteration: 330, Return: 200.0\n",
            "Iteration: 340, Return: 200.0\n",
            "Iteration: 350, Return: 200.0\n",
            "Iteration: 360, Return: 200.0\n",
            "Iteration: 370, Return: 200.0\n",
            "Iteration: 380, Return: 200.0\n",
            "Iteration: 390, Return: 200.0\n",
            "Iteration: 400, Return: 200.0\n",
            "Iteration: 410, Return: 200.0\n",
            "Iteration: 420, Return: 200.0\n",
            "Iteration: 430, Return: 200.0\n",
            "Iteration: 440, Return: 200.0\n",
            "Iteration: 450, Return: 200.0\n",
            "Iteration: 460, Return: 200.0\n",
            "Iteration: 470, Return: 200.0\n",
            "Iteration: 480, Return: 200.0\n",
            "Iteration: 490, Return: 109.0\n",
            "Iteration: 500, Return: 199.0\n",
            "Iteration: 510, Return: 200.0\n",
            "Iteration: 520, Return: 200.0\n",
            "Iteration: 530, Return: 112.0\n",
            "Iteration: 540, Return: 116.0\n",
            "Iteration: 550, Return: 112.0\n",
            "Iteration: 560, Return: 101.0\n",
            "Iteration: 570, Return: 200.0\n",
            "Iteration: 580, Return: 200.0\n",
            "Iteration: 590, Return: 200.0\n",
            "Iteration: 600, Return: 200.0\n",
            "Iteration: 610, Return: 200.0\n",
            "Iteration: 620, Return: 200.0\n",
            "Iteration: 630, Return: 200.0\n",
            "Iteration: 640, Return: 200.0\n",
            "Iteration: 650, Return: 200.0\n",
            "Iteration: 660, Return: 200.0\n",
            "Iteration: 670, Return: 200.0\n",
            "Iteration: 680, Return: 200.0\n",
            "Iteration: 690, Return: 200.0\n",
            "Iteration: 700, Return: 138.0\n",
            "Iteration: 710, Return: 118.0\n",
            "Iteration: 720, Return: 135.0\n",
            "Iteration: 730, Return: 123.0\n",
            "Iteration: 740, Return: 200.0\n",
            "Iteration: 750, Return: 200.0\n",
            "Iteration: 760, Return: 200.0\n",
            "Iteration: 770, Return: 200.0\n",
            "Iteration: 780, Return: 200.0\n",
            "Iteration: 790, Return: 200.0\n",
            "Iteration: 800, Return: 200.0\n",
            "Iteration: 810, Return: 200.0\n",
            "Iteration: 820, Return: 200.0\n",
            "Iteration: 830, Return: 200.0\n",
            "Iteration: 840, Return: 200.0\n",
            "Iteration: 850, Return: 200.0\n",
            "Iteration: 860, Return: 200.0\n",
            "Iteration: 870, Return: 200.0\n",
            "Iteration: 880, Return: 200.0\n",
            "Iteration: 890, Return: 200.0\n",
            "Iteration: 900, Return: 200.0\n",
            "Iteration: 910, Return: 200.0\n",
            "Iteration: 920, Return: 134.0\n",
            "Iteration: 930, Return: 122.0\n",
            "Iteration: 940, Return: 101.0\n",
            "Iteration: 950, Return: 45.0\n",
            "Iteration: 960, Return: 131.0\n",
            "Iteration: 970, Return: 116.0\n",
            "Iteration: 980, Return: 144.0\n",
            "Iteration: 990, Return: 168.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}