{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PPO Pytorch",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOJQWkbiQlyBZFFXs9iF6zb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dude123studios/AdvancedReinforcementLearning/blob/main/PPO_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-cL1xGWL7-J"
      },
      "source": [
        "import torch as T\n",
        "from torch import nn\n",
        "import torch.distributions as D \n",
        "from torch import optim as O\n",
        "import numpy as np\n",
        "import gym"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ag8HKdYkIeQC",
        "outputId": "5ffe2c5c-77f7-45c7-9066-efeef3625d8c"
      },
      "source": [
        "device = T.device('cpu')\n",
        "\n",
        "if(T.cuda.is_available()): \n",
        "    device = T.device('cuda:0') \n",
        "    T.cuda.empty_cache()\n",
        "    print(\"Device set to : \" + str(T.cuda.get_device_name(device)))\n",
        "else:\n",
        "    print(\"Device set to : cpu\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device set to : Tesla K80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iF4s8diUW-7z",
        "outputId": "15443435-8fbf-445a-89f5-006c3b9bef69"
      },
      "source": [
        "env = gym.make('Pendulum-v0')\n",
        "state_shape = env.observation_space.shape[0]\n",
        "action_shape = env.observation_space.shape[0]\n",
        "action_bound = [env.action_space.low, env.action_space.high]\n",
        "print(f'state shape: {state_shape}')\n",
        "print(f'action shape: {action_shape}')\n",
        "print(f'action bound: {action_bound}')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "state shape: 3\n",
            "action shape: 3\n",
            "action bound: [array([-2.], dtype=float32), array([2.], dtype=float32)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJBl5OIXbCiz"
      },
      "source": [
        "class PolicyNW(nn.Module):\n",
        "\n",
        "    def __init__(self, state_shape, action_shape):\n",
        "        super(PolicyNW, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_shape, 128)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fcmu = nn.Linear(128, action_shape)\n",
        "        self.mu_act = nn.Tanh()\n",
        "        self.fcsigma = nn.Linear(128, action_shape)\n",
        "        self.sigma_act = nn.Softplus()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.relu1(self.fc1(x))\n",
        "        mu = 2 * self.mu_act(self.fcmu(x))\n",
        "        sigma = self.sigma_act(self.fcsigma(x))\n",
        "\n",
        "        return mu, sigma\n",
        "\n",
        "pi = PolicyNW(state_shape, action_shape).to(device)\n",
        "old_pi = PolicyNW(state_shape, action_shape).to(device)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipE4h_4eLmBR"
      },
      "source": [
        "def update_old_pi():\n",
        "    old_pi.load_state_dict(pi.state_dict())\n",
        "\n",
        "update_old_pi()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Di0W3tLdFHT"
      },
      "source": [
        "v = nn.Sequential(nn.Linear(state_shape, 128), nn.ReLU(), nn.Linear(128, 1)).to(device)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMH5K2D5dYgk"
      },
      "source": [
        "pi_optim = O.Adam(pi.parameters(), lr=1e-3)\n",
        "v_optim = O.Adam(v.parameters(), lr=2e-3)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9itYAwKLew6"
      },
      "source": [
        "#Hyper Parameters\n",
        "num_episodes = 2000\n",
        "num_timesteps = 200\n",
        "gamma = 0.9\n",
        "delta = 0.3\n",
        "beta = 0.2\n",
        "epsilon = 0.2\n",
        "batch_size = 32\n",
        "epochs = 10"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJtm-ZS7L0lD"
      },
      "source": [
        "def policy(state):\n",
        "    with T.no_grad():\n",
        "        state = T.tensor(state, dtype=T.float32).unsqueeze(0).to(device)\n",
        "        mu, sigma = pi(state)\n",
        "        dist = D.Normal(mu[0], sigma[0])\n",
        "        action = dist.sample().unsqueeze(0)\n",
        "        clipped = T.clip(action, min=-2, max=2)[0]\n",
        "    \n",
        "    return clipped.cpu().numpy()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjT-uSvhOG4z"
      },
      "source": [
        "def value(state):\n",
        "    with T.no_grad():\n",
        "        state = T.tensor(state, dtype=T.float32).unsqueeze(0).to(device)\n",
        "        out = v(state)[0, 0]\n",
        "    return out.cpu().numpy()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOv5iZLvZkS9"
      },
      "source": [
        "def train_step(state, action, reward, _beta):\n",
        "\n",
        "    update_old_pi()\n",
        "\n",
        "    state = T.tensor(state, dtype=T.float32).to(device)\n",
        "    action = T.tensor(action, dtype=T.float32).to(device)\n",
        "    reward = T.tensor(reward, dtype=T.float32).to(device)\n",
        "\n",
        "    with T.no_grad():\n",
        "        advantage_cost = reward - v(state)\n",
        "\n",
        "    for _ in range(epochs):\n",
        "\n",
        "        mu, sigma = pi(state)\n",
        "        dist = D.Normal(mu, sigma)\n",
        "        pi_prob = dist.log_prob(action)\n",
        "\n",
        "        with T.no_grad():\n",
        "            mu_, sigma_ = old_pi(state)\n",
        "            dist_ = D.Normal(mu_, sigma_)\n",
        "            old_pi_prob = dist_.log_prob(action)\n",
        "\n",
        "        kl_div = nn.KLDivLoss()(pi_prob, old_pi_prob)\n",
        "\n",
        "        ratio = pi_prob/(old_pi_prob + 1e-8)\n",
        "        objective = ratio * advantage_cost\n",
        "\n",
        "        clipped = T.minimum(objective, T.clip(ratio, 1 - epsilon, 1+epsilon) * advantage_cost)\n",
        "\n",
        "        pi_loss = -T.mean(clipped - _beta * kl_div)\n",
        "\n",
        "        pi_optim.zero_grad()\n",
        "        pi_loss.backward()\n",
        "        pi_optim.step()\n",
        "    \n",
        "\n",
        "    mean_kl = T.mean(kl_div)\n",
        "    if mean_kl > 1.5 * delta:\n",
        "        _beta *= 2.0\n",
        "    elif mean_kl < delta/1.5:\n",
        "        _beta *= 0.5\n",
        "    \n",
        "    advantage = reward - v(state)\n",
        "    v_loss = T.mean(T.square(advantage))\n",
        "\n",
        "    v_optim.zero_grad()\n",
        "    v_loss.backward()\n",
        "    v_optim.step()\n",
        "\n",
        "    return _beta"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOxdDwRUmEEt",
        "outputId": "d70ff7e9-5cf3-484a-b7b3-be092c803bde"
      },
      "source": [
        "for i in range(1, num_episodes + 1):\n",
        "\n",
        "    state = env.reset()\n",
        "    episode_states, episode_actions, episode_rewards = [], [], []\n",
        "    Return = 0\n",
        "\n",
        "    for t in range(num_timesteps):\n",
        "\n",
        "        action = policy(state)\n",
        "\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        episode_states.append(state)\n",
        "        episode_rewards.append(reward)\n",
        "        episode_actions.append(action)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        Return += reward\n",
        "\n",
        "        if (t+1) % batch_size == 0 or t == num_timesteps - 1:\n",
        "\n",
        "            v_s_ = value(state)\n",
        "\n",
        "            discounted_r = []\n",
        "            for reward in episode_rewards[::-1]:\n",
        "                v_s_ = reward + gamma * v_s_\n",
        "                discounted_r.append(v_s_)\n",
        "            \n",
        "            discounted_r.reverse()\n",
        "\n",
        "            es, ea, er = np.vstack(episode_states), np.vstack(episode_actions), np.array(discounted_r, np.float32)[:, np.newaxis]\n",
        "\n",
        "            beta = train_step(es, ea, er, beta)\n",
        "\n",
        "            episode_states, episode_actions, episode_rewards = [], [], []\n",
        "    \n",
        "    if i % 10 == 0:\n",
        "        print('Episode: {}, Return: {}'.format(i, Return))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:2742: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 10, Return: -1522.7318403153251\n",
            "Episode: 20, Return: -1436.8637394367115\n",
            "Episode: 30, Return: -1342.8536676300869\n",
            "Episode: 40, Return: -1345.5870603192936\n",
            "Episode: 50, Return: -1274.18784320794\n",
            "Episode: 60, Return: -1190.8840683438277\n",
            "Episode: 70, Return: -1422.1157097649468\n",
            "Episode: 80, Return: -1183.722822864579\n",
            "Episode: 90, Return: -1200.5849702249411\n",
            "Episode: 100, Return: -1204.150827137444\n",
            "Episode: 110, Return: -1365.8312773984346\n",
            "Episode: 120, Return: -1310.4954128822549\n",
            "Episode: 130, Return: -1182.0792397321698\n",
            "Episode: 140, Return: -1295.447985118234\n",
            "Episode: 150, Return: -1227.4772388815893\n",
            "Episode: 160, Return: -1205.4716990149443\n",
            "Episode: 170, Return: -1517.9987064175868\n",
            "Episode: 180, Return: -1206.5832777223195\n",
            "Episode: 190, Return: -1310.6899235531148\n",
            "Episode: 200, Return: -1591.7302249432098\n",
            "Episode: 210, Return: -1508.1832235734726\n",
            "Episode: 220, Return: -1512.0088363106863\n",
            "Episode: 230, Return: -1431.5759301405528\n",
            "Episode: 240, Return: -1489.595996586204\n",
            "Episode: 250, Return: -1536.0098671702387\n",
            "Episode: 260, Return: -1388.32221920973\n",
            "Episode: 270, Return: -1478.8177177569696\n",
            "Episode: 280, Return: -1213.6677604879978\n",
            "Episode: 290, Return: -1284.0137612414078\n",
            "Episode: 300, Return: -1475.880426334496\n",
            "Episode: 310, Return: -1442.398070011562\n",
            "Episode: 320, Return: -1637.6622771404907\n",
            "Episode: 330, Return: -1627.605716152976\n",
            "Episode: 340, Return: -1520.5143322554084\n",
            "Episode: 350, Return: -1439.7092810531055\n",
            "Episode: 360, Return: -1500.5270398142743\n",
            "Episode: 370, Return: -1471.4256887113331\n",
            "Episode: 380, Return: -1397.684578881945\n",
            "Episode: 390, Return: -1279.8578678838117\n",
            "Episode: 400, Return: -1330.397649645391\n",
            "Episode: 410, Return: -1630.0538823536638\n",
            "Episode: 420, Return: -1255.4335410715798\n",
            "Episode: 430, Return: -1361.8841944717628\n",
            "Episode: 440, Return: -1306.5814898445701\n",
            "Episode: 450, Return: -1484.8423124800381\n",
            "Episode: 460, Return: -1587.8664743642073\n",
            "Episode: 470, Return: -1650.5906715537749\n",
            "Episode: 480, Return: -1534.9890564550712\n",
            "Episode: 490, Return: -1656.536208418101\n",
            "Episode: 500, Return: -1655.4699009499675\n",
            "Episode: 510, Return: -1430.2546867401163\n",
            "Episode: 520, Return: -1475.879302767051\n",
            "Episode: 530, Return: -1492.3107827287763\n",
            "Episode: 540, Return: -1223.0445179411136\n",
            "Episode: 550, Return: -1471.3709283913524\n",
            "Episode: 560, Return: -1403.6344505677796\n",
            "Episode: 570, Return: -1449.6705324430468\n",
            "Episode: 580, Return: -1471.1894220577847\n",
            "Episode: 590, Return: -1560.2436141391788\n",
            "Episode: 600, Return: -1644.784164017505\n",
            "Episode: 610, Return: -1539.9490775728475\n",
            "Episode: 620, Return: -1438.8235263261558\n",
            "Episode: 630, Return: -1469.5640680779106\n",
            "Episode: 640, Return: -1492.8380262504345\n",
            "Episode: 650, Return: -1657.4085089908585\n",
            "Episode: 660, Return: -1657.3190493493946\n",
            "Episode: 670, Return: -1646.8455558395449\n",
            "Episode: 680, Return: -1623.1075414140566\n",
            "Episode: 690, Return: -1512.3366317696198\n",
            "Episode: 700, Return: -1334.3720672127083\n",
            "Episode: 710, Return: -1514.8719706893994\n",
            "Episode: 720, Return: -1498.3334784711253\n",
            "Episode: 730, Return: -1574.8035519302828\n",
            "Episode: 740, Return: -1329.6864156302022\n",
            "Episode: 750, Return: -1323.229431698006\n",
            "Episode: 760, Return: -1439.7597697764513\n",
            "Episode: 770, Return: -1655.783089588842\n",
            "Episode: 780, Return: -866.6427997377114\n",
            "Episode: 790, Return: -1560.8799108221453\n",
            "Episode: 800, Return: -1303.4731131050198\n",
            "Episode: 810, Return: -1238.604010673407\n",
            "Episode: 820, Return: -1488.1076445717263\n",
            "Episode: 830, Return: -1454.5588262510748\n",
            "Episode: 840, Return: -1781.6077664653608\n",
            "Episode: 850, Return: -1515.3397743710245\n",
            "Episode: 860, Return: -1838.8077492705095\n",
            "Episode: 870, Return: -1751.3905084502326\n",
            "Episode: 880, Return: -1743.9525178873066\n",
            "Episode: 890, Return: -1013.2986291447085\n",
            "Episode: 900, Return: -1588.4110892666627\n",
            "Episode: 910, Return: -1511.6288580921544\n",
            "Episode: 920, Return: -1792.3091406104068\n",
            "Episode: 930, Return: -1546.1161083148502\n",
            "Episode: 940, Return: -1825.291130613871\n",
            "Episode: 950, Return: -1852.179872476112\n",
            "Episode: 960, Return: -1701.8630085669768\n",
            "Episode: 970, Return: -1452.0022358181436\n",
            "Episode: 980, Return: -1706.2588059270304\n",
            "Episode: 990, Return: -1355.7056874961258\n",
            "Episode: 1000, Return: -1522.4523832028365\n",
            "Episode: 1010, Return: -1063.2417937064652\n",
            "Episode: 1020, Return: -1465.0519585842094\n",
            "Episode: 1030, Return: -994.7404269644203\n",
            "Episode: 1040, Return: -1677.7391409510321\n",
            "Episode: 1050, Return: -1263.2166650730258\n",
            "Episode: 1060, Return: -1328.2748304433142\n",
            "Episode: 1070, Return: -1869.7020302269764\n",
            "Episode: 1080, Return: -1401.7874263062065\n",
            "Episode: 1090, Return: -1536.944510410135\n",
            "Episode: 1100, Return: -1604.809550513974\n",
            "Episode: 1110, Return: -1648.4835358432517\n",
            "Episode: 1120, Return: -1537.4905973994746\n",
            "Episode: 1130, Return: -1373.3441016421\n",
            "Episode: 1140, Return: -1598.994782316598\n",
            "Episode: 1150, Return: -1510.762833835236\n",
            "Episode: 1160, Return: -1715.4697437834498\n",
            "Episode: 1170, Return: -1619.0913096394881\n",
            "Episode: 1180, Return: -1551.80355524119\n",
            "Episode: 1190, Return: -1636.6855369862387\n",
            "Episode: 1200, Return: -1566.9930077218148\n",
            "Episode: 1210, Return: -1857.8553921301493\n",
            "Episode: 1220, Return: -1515.216823017601\n",
            "Episode: 1230, Return: -1538.9327299198947\n",
            "Episode: 1240, Return: -1616.579270947697\n",
            "Episode: 1250, Return: -1635.6650298708469\n",
            "Episode: 1260, Return: -1645.051871304906\n",
            "Episode: 1270, Return: -1654.5433580621452\n",
            "Episode: 1280, Return: -1654.7554020158016\n",
            "Episode: 1290, Return: -1587.560078199167\n",
            "Episode: 1300, Return: -1562.6544160255146\n",
            "Episode: 1310, Return: -1490.1104920260825\n",
            "Episode: 1320, Return: -1505.4883048978454\n",
            "Episode: 1330, Return: -1488.5127221582807\n",
            "Episode: 1340, Return: -1535.9746845652837\n",
            "Episode: 1350, Return: -1372.9710391599024\n",
            "Episode: 1360, Return: -1562.5971034712948\n",
            "Episode: 1370, Return: -1507.9882875565113\n",
            "Episode: 1380, Return: -1609.7516364915516\n",
            "Episode: 1390, Return: -1631.1606981023265\n",
            "Episode: 1400, Return: -1494.4445436528952\n",
            "Episode: 1410, Return: -1549.1899227685724\n",
            "Episode: 1420, Return: -1613.6379450976253\n",
            "Episode: 1430, Return: -1489.470514951827\n",
            "Episode: 1440, Return: -1384.4370956703024\n",
            "Episode: 1450, Return: -1428.5625846517046\n",
            "Episode: 1460, Return: -1532.5286116811928\n",
            "Episode: 1470, Return: -1515.149439703761\n",
            "Episode: 1480, Return: -1611.3175173772615\n",
            "Episode: 1490, Return: -1612.9772127172573\n",
            "Episode: 1500, Return: -1613.0835998224452\n",
            "Episode: 1510, Return: -1649.034234278793\n",
            "Episode: 1520, Return: -1517.2367266107515\n",
            "Episode: 1530, Return: -1627.416752404577\n",
            "Episode: 1540, Return: -1576.9513790908659\n",
            "Episode: 1550, Return: -1594.2068588178715\n",
            "Episode: 1560, Return: -1644.6897174996602\n",
            "Episode: 1570, Return: -1554.7381456693206\n",
            "Episode: 1580, Return: -1525.1897900207275\n",
            "Episode: 1590, Return: -1539.6264857405813\n",
            "Episode: 1600, Return: -1358.3265183056344\n",
            "Episode: 1610, Return: -1526.3205450683665\n",
            "Episode: 1620, Return: -1396.4202891056527\n",
            "Episode: 1630, Return: -1577.3153924224625\n",
            "Episode: 1640, Return: -1568.8565263167661\n",
            "Episode: 1650, Return: -1381.9700945706454\n",
            "Episode: 1660, Return: -1418.8420262058999\n",
            "Episode: 1670, Return: -1558.3614043686991\n",
            "Episode: 1680, Return: -1559.6650420741107\n",
            "Episode: 1690, Return: -1570.6571997639005\n",
            "Episode: 1700, Return: -1541.7752970609663\n",
            "Episode: 1710, Return: -1345.4656415336115\n",
            "Episode: 1720, Return: -1176.2624294980194\n",
            "Episode: 1730, Return: -1860.8584706350696\n",
            "Episode: 1740, Return: -1271.9546065300242\n",
            "Episode: 1750, Return: -1229.7106726418085\n",
            "Episode: 1760, Return: -1252.753178703422\n",
            "Episode: 1770, Return: -1286.657212616196\n",
            "Episode: 1780, Return: -1222.6644956601065\n",
            "Episode: 1790, Return: -1303.2371655248442\n",
            "Episode: 1800, Return: -1338.8176874743885\n",
            "Episode: 1810, Return: -1331.726028139481\n",
            "Episode: 1820, Return: -1357.062852192737\n",
            "Episode: 1830, Return: -1351.361424871063\n",
            "Episode: 1840, Return: -1384.0938105648465\n",
            "Episode: 1850, Return: -1637.516574734951\n",
            "Episode: 1860, Return: -1566.0896345281546\n",
            "Episode: 1870, Return: -1454.0821178728957\n",
            "Episode: 1880, Return: -1435.2187972807944\n",
            "Episode: 1890, Return: -1541.0963367045608\n",
            "Episode: 1900, Return: -1457.6231432918942\n",
            "Episode: 1910, Return: -1368.5928730106948\n",
            "Episode: 1920, Return: -1505.9139273477135\n",
            "Episode: 1930, Return: -1501.9315596063107\n",
            "Episode: 1940, Return: -1537.8333495713885\n",
            "Episode: 1950, Return: -1622.3126691300058\n",
            "Episode: 1960, Return: -1519.3669631734267\n",
            "Episode: 1970, Return: -1909.7633420520492\n",
            "Episode: 1980, Return: -1472.5902554716497\n",
            "Episode: 1990, Return: -1648.031536065775\n",
            "Episode: 2000, Return: -1539.8725536552731\n"
          ]
        }
      ]
    }
  ]
}