{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DDPG",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMvsGWLq6eAAG4UUn7nzf6I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dude123studios/AdvancedReinforcementLearning/blob/main/DDPG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdE3-xrJtpKb",
        "outputId": "42f66118-2340-4f71-af5d-94a3aa2ae43d"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "import gym"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dc-KCt3st7eC"
      },
      "source": [
        "env = gym.make(\"Pendulum-v0\")\n",
        "state_shape = env.observation_space.shape[0]\n",
        "action_shape = env.action_space.shape[0]\n",
        "action_bound = [env.action_space.low, env.action_space.high]\n",
        "gamma = 0.9\n",
        "tau = 0.001\n",
        "replay_buffer = 10000\n",
        "batch_size = 32"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8zA-vxLt7ch"
      },
      "source": [
        "class DDPG(object):\n",
        "\n",
        "    def __init__(self, state_shape, action_shape, high_action_value):\n",
        "        #s, a, s', r\n",
        "        self.replay_buffer = np.zeros((replay_buffer, state_shape * 2 + action_shape + 1), dtype=np.float32)\n",
        "        self.num_transitions = 0\n",
        "        \n",
        "        self.sess = tf.Session()\n",
        "        self.noise = 3.0\n",
        "\n",
        "        self.state_shape, self.action_shape, self.high_action_value = state_shape, action_shape, high_action_value\n",
        "\n",
        "        self.state = tf.placeholder(tf.float32, [None, state_shape], 'state')\n",
        "\n",
        "        #For critic loss\n",
        "        self.next_state = tf.placeholder(tf.float32, [None, state_shape], 'next_state')\n",
        "        self.reward = tf.placeholder(tf.float32, [None, 1], 'reward')\n",
        "\n",
        "        with tf.variable_scope('Actor'):\n",
        "\n",
        "            self.actor = self.build_actor_network(self.state, scope='main', trainable=True)\n",
        "            target_actor = self.build_actor_network(self.next_state, scope='target', trainable=False)\n",
        "        \n",
        "        with tf.variable_scope('Critic'):\n",
        "            \n",
        "            critic = self.build_critic_network(self.state, self.actor, scope='main', trainable=False)\n",
        "            target_critic = self.build_critic_network(self.next_state, target_actor, scope='target', trainable=False)\n",
        "        \n",
        "        self.main_actor_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/main')\n",
        "        self.target_actor_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/target')\n",
        "\n",
        "        self.main_critic_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/main')\n",
        "        self.target_critic_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/target')\n",
        "\n",
        "        self.soft_replacement = [\n",
        "            [tf.assign(phi_, tau*phi + (1-tau) * phi_), tf.assign(theta_, tau*theta + (1-tau)*theta_)]\n",
        "            for phi, phi_, theta, theta_ in zip(self.main_actor_params, self.target_actor_params,\n",
        "                                                self.main_critic_params, self.target_critic_params)\n",
        "        ]\n",
        "\n",
        "        #Critic Losses\n",
        "        y = self.reward + gamma * target_critic\n",
        "        MSE = tf.losses.mean_squared_error(labels=y, predictions=critic)\n",
        "\n",
        "        self.train_critic = tf.train.AdamOptimizer(1e-2).minimize(MSE, name='adam-ink', var_list = self.main_critic_params)\n",
        "\n",
        "        #Actor Losses\n",
        "        actor_loss = -tf.reduce_mean(critic)\n",
        "\n",
        "        self.train_actor = tf.train.AdamOptimizer(1e-3).minimize(actor_loss, var_list=self.main_actor_params)\n",
        "\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    def select_action(self, state):\n",
        "\n",
        "        action = self.sess.run(self.actor, {self.state: state[np.newaxis, :]})[0]\n",
        "        action = np.random.normal(action, self.noise)\n",
        "\n",
        "        action = np.clip(action, action_bound[0], action_bound[1])\n",
        "        return action\n",
        "        \n",
        "    def train(self):\n",
        "            \n",
        "        #Soft replacement\n",
        "        self.sess.run(self.soft_replacement)\n",
        "\n",
        "        #Extract batch\n",
        "        indices = np.random.choice(replay_buffer, size=batch_size)\n",
        "        batch_transitions = self.replay_buffer[indices, :]\n",
        "        batch_states = batch_transitions[:, :self.state_shape]\n",
        "        batch_actions = batch_transitions[:, self.state_shape: self.state_shape + self.action_shape]\n",
        "        batch_rewards = batch_transitions[:, -self.state_shape-1: -self.state_shape]\n",
        "        batch_next_state = batch_transitions[:, -self.state_shape:]\n",
        "\n",
        "\n",
        "        self.sess.run(self.train_actor, {self.state: batch_states})\n",
        "        self.sess.run(self.train_critic, {self.state: batch_states, \n",
        "                                            self.actor: batch_actions, \n",
        "                                            self.reward: batch_rewards, \n",
        "                                            self.next_state: batch_next_state})\n",
        "        \n",
        "    def store_transition(self, state, actor, reward, next_state):\n",
        "\n",
        "        trans = np.hstack((state, actor, [reward], next_state))\n",
        "        index = self.num_transitions % replay_buffer\n",
        "        self.replay_buffer[index, :] = trans\n",
        "        self.num_transitions += 1\n",
        "\n",
        "        if self.num_transitions > replay_buffer:\n",
        "            self.noise *= 0.99995\n",
        "            self.train()\n",
        "        \n",
        "    def build_actor_network(self, state, scope, trainable):\n",
        "            \n",
        "        with tf.variable_scope(scope):\n",
        "            layer_1 = tf.layers.dense(state, 30, activation = tf.nn.tanh, name = 'layer_1', trainable=trainable)\n",
        "            actor = tf.layers.dense(layer_1, self.action_shape, activation = tf.nn.tanh, name='actor', trainable = trainable)\n",
        "            return tf.multiply(actor, self.high_action_value, name = 'scaled_a')\n",
        "\n",
        "    def build_critic_network(self, state, actor, scope, trainable):\n",
        "\n",
        "        with tf.variable_scope(scope):\n",
        "\n",
        "            w1_s = tf.get_variable('w1_s', [self.state_shape, 30], trainable = trainable)\n",
        "            w1_a = tf.get_variable('w1_a', [self.action_shape, 30], trainable = trainable)\n",
        "            b1 = tf.get_variable('b1', [1, 30], trainable = trainable)\n",
        "\n",
        "            net = tf.nn.tanh(tf.matmul(state, w1_s) + tf.matmul(actor, w1_a) + b1)\n",
        "            critic = tf.layers.dense(net, 1, trainable = trainable)\n",
        "            return critic"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWBSOMxLhFVL",
        "outputId": "97664740-64a0-4535-ce2e-061718d7d923"
      },
      "source": [
        "ddpg = DDPG(state_shape, action_shape, action_bound[1])\n",
        "\n",
        "num_episodes = 300\n",
        "num_timesteps = 500\n",
        "\n",
        "for i in range(num_episodes):\n",
        "\n",
        "    state = env.reset()\n",
        "\n",
        "    Return = 0\n",
        "\n",
        "    for t in range(num_timesteps):\n",
        "\n",
        "        action = ddpg.select_action(state)\n",
        "\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        \n",
        "        ddpg.store_transition(state, action, reward, next_state)\n",
        "\n",
        "        Return += reward\n",
        "\n",
        "        if done:\n",
        "            break \n",
        "        \n",
        "        state = next_state \n",
        "\n",
        "    if i % 10 == 0:\n",
        "        print('Episode: {}, Return: {}'.format(i, Return))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 0, Return: -1679.3030325142167\n",
            "Episode: 10, Return: -1311.5199316641963\n",
            "Episode: 20, Return: -1728.6373369572862\n",
            "Episode: 30, Return: -1472.5554317414294\n",
            "Episode: 40, Return: -1764.6135696286958\n",
            "Episode: 50, Return: -1835.276100881685\n",
            "Episode: 60, Return: -1173.6981367177168\n",
            "Episode: 70, Return: -1067.6976875713656\n",
            "Episode: 80, Return: -1052.4688327617355\n",
            "Episode: 90, Return: -1144.3510885708486\n",
            "Episode: 100, Return: -1248.015076254429\n",
            "Episode: 110, Return: -883.0987145094817\n",
            "Episode: 120, Return: -1033.701899158679\n",
            "Episode: 130, Return: -130.91144346901925\n",
            "Episode: 140, Return: -387.7060604572675\n",
            "Episode: 150, Return: -423.52970043743255\n",
            "Episode: 160, Return: -283.7459638825973\n",
            "Episode: 170, Return: -282.00079647462735\n",
            "Episode: 180, Return: -408.12087472426055\n",
            "Episode: 190, Return: -390.5024248584339\n",
            "Episode: 200, Return: -129.15696445425166\n",
            "Episode: 210, Return: -129.93822367321079\n",
            "Episode: 220, Return: -0.7278130629575704\n",
            "Episode: 230, Return: -378.8301946233798\n",
            "Episode: 240, Return: -126.06049298656808\n",
            "Episode: 250, Return: -133.0321453832666\n",
            "Episode: 260, Return: -271.1117440816917\n",
            "Episode: 270, Return: -247.31441642227324\n",
            "Episode: 280, Return: -1.0954333562011322\n",
            "Episode: 290, Return: -432.8914389016345\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}