{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Distributional RL (C51)",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO4+uO3LcVLaL0oQ0aa2WER",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dude123studios/AdvancedReinforcementLearning/blob/main/Distributional_RL_(C51).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_zFERT0iysc"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "import gym"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dSVhhITNJE3"
      },
      "source": [
        "!wget http://www.atarimania.com/roms/Roms.rar\n",
        "!unrar x Roms.rar\n",
        "!unzip ROMS.zip\n",
        "!python -m atari_py.import_roms ROMS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lFqmNArkEZS"
      },
      "source": [
        "#Hyper Params\n",
        "v_min = 0\n",
        "v_max = 1000\n",
        "atoms = 51\n",
        "gamma = 0.9\n",
        "batch_size = 64\n",
        "update_target_net = 50\n",
        "epsilon = 0.5\n",
        "\n",
        "buffer_length=20000\n",
        "replay_buffer = deque(maxlen=buffer_length)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kM6yKy1BqD0"
      },
      "source": [
        "env = gym.make('Tennis-v0')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PM-xEqdChxK"
      },
      "source": [
        "class CategoricalDQN(object):\n",
        "\n",
        "    def __init__(self, env):\n",
        "        \n",
        "        self.time_step = 0\n",
        "\n",
        "        self.v_min = v_min\n",
        "        self.v_max = v_max\n",
        "\n",
        "        self.state_shape = env.observation_space.shape\n",
        "        self.action_shape = env.action_space.n\n",
        "\n",
        "        self.atoms = 51\n",
        "\n",
        "        self.delta_z = (v_max - v_min) / (self.atoms - 1)\n",
        "\n",
        "        self.z = tf.convert_to_tensor(np.array([[self.v_min + i * self.delta_z for i in range(self.atoms)]]))\n",
        "        self.z = tf.cast(self.z, tf.float32)\n",
        "\n",
        "        self.main = self.build_network()\n",
        "        self.target = self.build_network()\n",
        "\n",
        "        self.optimizer = tf.keras.optimizers.Adam(1e-3)\n",
        "\n",
        "    \n",
        "    def build_network(self):\n",
        "\n",
        "        state = Input(self.state_shape, dtype=tf.float32, name=\"state\")\n",
        "        action = Input((1,), dtype=tf.float32, name=\"action\")\n",
        "\n",
        "        x = Conv2D(6, (5, 5), (2, 2), padding='same')(state)\n",
        "        x = Conv2D(12, (3, 3), (2, 2), padding='same')(x)\n",
        "        x = Flatten()(x)\n",
        "\n",
        "        x = Dense(24, 'relu')(x)\n",
        "        x = Dense(24, 'relu')(x)\n",
        "        concat = Concatenate(axis=-1)([x, tf.reshape(action, (1, 1))])\n",
        "\n",
        "        x = Dense(atoms, activation='softmax')(concat)\n",
        "\n",
        "        return tf.keras.Model(inputs=[state, action], outputs=x)\n",
        "    \n",
        "    def target_q(self, state, action):\n",
        "        probs = self.target([state, tf.convert_to_tensor(np.array([[action]]))])\n",
        "        return tf.reduce_sum(probs * self.z)\n",
        "    \n",
        "    def main_q(self, state, action):\n",
        "        probs = self.main([state, tf.convert_to_tensor(np.array([[action]]))])\n",
        "        return tf.reduce_sum(probs * self.z)\n",
        "\n",
        "    def train(self, s, r, action, s_):\n",
        "\n",
        "        self.time_step += 1\n",
        "\n",
        "        s = tf.convert_to_tensor(np.array([s]))\n",
        "        s_ = tf.convert_to_tensor(np.array([s_]))\n",
        "        action = tf.convert_to_tensor(np.array([action]))\n",
        "\n",
        "        target_q_value = np.array([self.target_q(s, a).numpy() for a in range(self.action_shape)])\n",
        "\n",
        "        a_ = np.argmax(target_q_value)\n",
        "\n",
        "        m = np.zeros(self.atoms)\n",
        "\n",
        "        p = self.target([s_, tf.convert_to_tensor(a_)]).numpy()[0]\n",
        "\n",
        "        for j in range(self.atoms):\n",
        "            Tz = min(self.v_max, max(self.v_min, r + gamma * self.z[0, j].numpy()))\n",
        "            bj = (Tz - self.v_min) / self.delta_z \n",
        "            l,u = math.floor(bj),math.ceil(bj) \n",
        "\n",
        "            pj = p[j]\n",
        "\n",
        "            m[int(l)] += pj * (u - bj)\n",
        "            m[int(u)] += pj * (bj - l)\n",
        "        \n",
        "        self.update_network(s, action, tf.convert_to_tensor(m))\n",
        "\n",
        "        if self.time_step % update_target_net == 0:\n",
        "            self.update_target_network()\n",
        "    \n",
        "    @tf.function\n",
        "    def update_network(self, state, action, m):\n",
        "        m = tf.expand_dims(m,  axis=0)\n",
        "        m = tf.cast(m, tf.float32)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            probs = self.main([state, action])\n",
        "\n",
        "            loss = m * tf.math.log(probs)\n",
        "        \n",
        "        grads = tape.gradient(loss, self.main.trainable_variables)\n",
        "\n",
        "        self.optimizer.apply_gradients(zip(grads, self.main.trainable_variables))\n",
        "    \n",
        "    def update_target_network(self):\n",
        "        self.target.set_weights(self.main.get_weights())\n",
        "        \n",
        "\n",
        "    def select_action(self, state):\n",
        "        if random.random() < epsilon:\n",
        "            return random.randint(0, self.action_shape - 1)\n",
        "        else:\n",
        "            if state.ndim < 4: state = state[np.newaxis, :]\n",
        "            state = tf.convert_to_tensor(state)\n",
        "            state = tf.cast(state, tf.float32)\n",
        "            q_values = np.array([self.main_q(state, a).numpy() for a in range(self.action_shape)])\n",
        "            return np.argmax(q_values)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZycOYu0Blw0s"
      },
      "source": [
        "def sample_transitions(batch_size):\n",
        "    batch = np.random.permutation(len(replay_buffer))[:batch_size]\n",
        "    trans = np.array(replay_buffer)[batch]\n",
        "    return trans"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7QiZdUp8tPn"
      },
      "source": [
        "agent = CategoricalDQN(env)\n",
        "\n",
        "num_episodes = 800\n",
        "\n",
        "for i in range(num_episodes):\n",
        "\n",
        "    done = False\n",
        "\n",
        "    Return = 0\n",
        "\n",
        "    state = env.reset()\n",
        "\n",
        "    while not done:\n",
        "\n",
        "        action = agent.select_action(state)\n",
        "\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        Return += reward\n",
        "\n",
        "        replay_buffer.append([state, reward, [action], next_state])\n",
        "\n",
        "        if len(replay_buffer) >= batch_size:\n",
        "\n",
        "            trans = sample_transitions(2)\n",
        "\n",
        "            for item in trans:\n",
        "                \n",
        "                agent.train(item[0], item[1], item[2], item[3])\n",
        "        \n",
        "        state = next_state\n",
        "    \n",
        "    print('Episode: {}, Return: {}'.format(i, Return))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}