{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN for Atari Pacman",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO+l7BXrRZ/1oFRPrmSlNcl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dude123studios/AdvancedReinforcementLearning/blob/main/Deep%20Q%20networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BoyEg0uenbP"
      },
      "source": [
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque \n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDXExFBMfOes"
      },
      "source": [
        "env = gym.make('MsPacman-v0')\n",
        "\n",
        "state_size = (88, 80, 1)\n",
        "action_size = env.action_space.n\n",
        "\n",
        "color = np.array([120, 164, 74]).mean()\n",
        "\n",
        "def preprocess_state(state):\n",
        "    #resize\n",
        "    image = state[1:176:2, ::2]\n",
        "\n",
        "    #convert to grayscale\n",
        "    image = image.mean(axis=2)\n",
        "    \n",
        "    #Improve contrast\n",
        "    image[image==color] = 0\n",
        "\n",
        "    #Normalize\n",
        "    image = (image - 128) / 128 - 1\n",
        "\n",
        "    image = np.expand_dims(image.reshape(88, 80, 1), axis = 0)\n",
        "\n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D12_aqOlkfx4"
      },
      "source": [
        "class DQN:\n",
        "\n",
        "    def __init__(self, state_size, action_size):\n",
        "\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        self.replay_buffer = deque(maxlen=5000)\n",
        "\n",
        "        self.gamma = 0.9\n",
        "\n",
        "        self.epsilon = 0.8\n",
        "\n",
        "        self.update_rate = 5\n",
        "\n",
        "        self.main_network = self.build_network()\n",
        "            \n",
        "        self.target_network = tf.keras.models.clone_model(self.main_network)\n",
        "    \n",
        "    def build_network(self):\n",
        "\n",
        "        model = Sequential([\n",
        "            Conv2D(32, (8, 8), strides=4, padding='same', input_shape=self.state_size),\n",
        "            Activation('relu'),\n",
        "\n",
        "            Conv2D(64, (4, 4), strides=2, padding='same'),\n",
        "            Activation('relu'),\n",
        "            \n",
        "            Conv2D(64, (3, 3), strides=1, padding='same'),\n",
        "            Activation('relu'),\n",
        "\n",
        "            Flatten(),\n",
        "\n",
        "            Dense(512, activation='relu'),\n",
        "            Dense(self.action_size, activation='linear')\n",
        "        ])\n",
        "\n",
        "        model.compile(loss='mse', optimizer=Adam())\n",
        "\n",
        "        return model\n",
        "    \n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
        "    \n",
        "    def epsilon_greedy(self, state):\n",
        "\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return np.random.randint(self.action_size)\n",
        "        \n",
        "        Q_values = self.main_network.predict(state)\n",
        "        return np.argmax(Q_values)\n",
        "    \n",
        "    def train(self, batch_size):\n",
        "\n",
        "        minibatch = random.sample(self.replay_buffer, batch_size)\n",
        "\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            \n",
        "            if not done:\n",
        "                target_Q = reward + self.gamma * np.amax(self.target_network.predict(next_state))\n",
        "            \n",
        "            else:\n",
        "                target_Q = reward\n",
        "            \n",
        "            Q_values = self.main_network.predict(state)\n",
        "            Q_values[0][action] = target_Q\n",
        "\n",
        "            self.main_network.fit(state, Q_values, epochs=1, verbose=0)\n",
        "\n",
        "    def update_target_network(self):\n",
        "        self.target_network.set_weights(self.main_network.get_weights()) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoWDAX5BsCg-"
      },
      "source": [
        "num_episodes = 200\n",
        "num_timesteps = 2000\n",
        "\n",
        "batch_size = 8\n",
        "num_screens = 4\n",
        "\n",
        "dqn = DQN(state_size, action_size)\n",
        "\n",
        "done = False\n",
        "\n",
        "time_step = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "i8L-MNfhwbqN",
        "outputId": "dc4e22b8-a665-400b-cee0-adb2a73a1124"
      },
      "source": [
        "for i in range(num_episodes):\n",
        "\n",
        "    Return = 0\n",
        "\n",
        "    state = preprocess_state(env.reset())\n",
        "    \n",
        "    time_step += 1\n",
        "\n",
        "    for t in range(num_timesteps):\n",
        "\n",
        "        #env.render()\n",
        "        \n",
        "        action = dqn.epsilon_greedy(state)\n",
        "\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        next_state = preprocess_state(next_state)\n",
        "\n",
        "        dqn.store_transition(state, action, reward, next_state, done)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        Return += reward\n",
        "\n",
        "        if done:\n",
        "            print('Episode: ', i, ', ' 'Return', Return)\n",
        "            break\n",
        "        \n",
        "    if len(dqn.replay_buffer) > batch_size:\n",
        "        dqn.train(batch_size)\n",
        "    \n",
        "    if time_step % dqn.update_rate == 0:\n",
        "            dqn.update_target_network()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode:  0 , Return 290.0\n",
            "Episode:  1 , Return 250.0\n",
            "Episode:  2 , Return 590.0\n",
            "Episode:  3 , Return 200.0\n",
            "Episode:  4 , Return 240.0\n",
            "Episode:  5 , Return 140.0\n",
            "Episode:  6 , Return 540.0\n",
            "Episode:  7 , Return 350.0\n",
            "Episode:  8 , Return 200.0\n",
            "Episode:  9 , Return 430.0\n",
            "Episode:  10 , Return 180.0\n",
            "Episode:  11 , Return 220.0\n",
            "Episode:  12 , Return 280.0\n",
            "Episode:  13 , Return 460.0\n",
            "Episode:  14 , Return 240.0\n",
            "Episode:  15 , Return 320.0\n",
            "Episode:  16 , Return 260.0\n",
            "Episode:  17 , Return 230.0\n",
            "Episode:  18 , Return 190.0\n",
            "Episode:  19 , Return 320.0\n",
            "Episode:  20 , Return 160.0\n",
            "Episode:  21 , Return 260.0\n",
            "Episode:  22 , Return 360.0\n",
            "Episode:  23 , Return 180.0\n",
            "Episode:  24 , Return 300.0\n",
            "Episode:  25 , Return 730.0\n",
            "Episode:  26 , Return 220.0\n",
            "Episode:  27 , Return 380.0\n",
            "Episode:  28 , Return 230.0\n",
            "Episode:  29 , Return 180.0\n",
            "Episode:  30 , Return 310.0\n",
            "Episode:  31 , Return 180.0\n",
            "Episode:  32 , Return 260.0\n",
            "Episode:  33 , Return 200.0\n",
            "Episode:  34 , Return 280.0\n",
            "Episode:  35 , Return 450.0\n",
            "Episode:  36 , Return 220.0\n",
            "Episode:  37 , Return 290.0\n",
            "Episode:  38 , Return 140.0\n",
            "Episode:  39 , Return 220.0\n",
            "Episode:  40 , Return 230.0\n",
            "Episode:  41 , Return 190.0\n",
            "Episode:  42 , Return 200.0\n",
            "Episode:  43 , Return 220.0\n",
            "Episode:  44 , Return 610.0\n",
            "Episode:  45 , Return 240.0\n",
            "Episode:  46 , Return 260.0\n",
            "Episode:  47 , Return 170.0\n",
            "Episode:  48 , Return 230.0\n",
            "Episode:  49 , Return 200.0\n",
            "Episode:  50 , Return 300.0\n",
            "Episode:  51 , Return 280.0\n",
            "Episode:  52 , Return 330.0\n",
            "Episode:  53 , Return 300.0\n",
            "Episode:  54 , Return 260.0\n",
            "Episode:  55 , Return 180.0\n",
            "Episode:  56 , Return 280.0\n",
            "Episode:  57 , Return 610.0\n",
            "Episode:  58 , Return 590.0\n",
            "Episode:  59 , Return 230.0\n",
            "Episode:  60 , Return 210.0\n",
            "Episode:  61 , Return 290.0\n",
            "Episode:  62 , Return 120.0\n",
            "Episode:  63 , Return 180.0\n",
            "Episode:  64 , Return 170.0\n",
            "Episode:  65 , Return 160.0\n",
            "Episode:  66 , Return 280.0\n",
            "Episode:  67 , Return 540.0\n",
            "Episode:  68 , Return 300.0\n",
            "Episode:  69 , Return 250.0\n",
            "Episode:  70 , Return 350.0\n",
            "Episode:  71 , Return 270.0\n",
            "Episode:  72 , Return 260.0\n",
            "Episode:  73 , Return 270.0\n",
            "Episode:  74 , Return 320.0\n",
            "Episode:  75 , Return 290.0\n",
            "Episode:  76 , Return 300.0\n",
            "Episode:  77 , Return 540.0\n",
            "Episode:  78 , Return 130.0\n",
            "Episode:  79 , Return 150.0\n",
            "Episode:  80 , Return 150.0\n",
            "Episode:  81 , Return 220.0\n",
            "Episode:  82 , Return 200.0\n",
            "Episode:  83 , Return 270.0\n",
            "Episode:  84 , Return 200.0\n",
            "Episode:  85 , Return 180.0\n",
            "Episode:  86 , Return 280.0\n",
            "Episode:  87 , Return 1000.0\n",
            "Episode:  88 , Return 170.0\n",
            "Episode:  89 , Return 460.0\n",
            "Episode:  90 , Return 270.0\n",
            "Episode:  91 , Return 310.0\n",
            "Episode:  92 , Return 150.0\n",
            "Episode:  93 , Return 190.0\n",
            "Episode:  94 , Return 100.0\n",
            "Episode:  95 , Return 190.0\n",
            "Episode:  96 , Return 190.0\n",
            "Episode:  97 , Return 200.0\n",
            "Episode:  98 , Return 150.0\n",
            "Episode:  99 , Return 230.0\n",
            "Episode:  100 , Return 170.0\n",
            "Episode:  101 , Return 200.0\n",
            "Episode:  102 , Return 260.0\n",
            "Episode:  103 , Return 200.0\n",
            "Episode:  104 , Return 70.0\n",
            "Episode:  105 , Return 690.0\n",
            "Episode:  106 , Return 280.0\n",
            "Episode:  107 , Return 140.0\n",
            "Episode:  108 , Return 210.0\n",
            "Episode:  109 , Return 260.0\n",
            "Episode:  110 , Return 270.0\n",
            "Episode:  111 , Return 320.0\n",
            "Episode:  112 , Return 220.0\n",
            "Episode:  113 , Return 210.0\n",
            "Episode:  114 , Return 180.0\n",
            "Episode:  115 , Return 230.0\n",
            "Episode:  116 , Return 250.0\n",
            "Episode:  117 , Return 1040.0\n",
            "Episode:  118 , Return 210.0\n",
            "Episode:  119 , Return 210.0\n",
            "Episode:  120 , Return 210.0\n",
            "Episode:  121 , Return 260.0\n",
            "Episode:  122 , Return 330.0\n",
            "Episode:  123 , Return 340.0\n",
            "Episode:  124 , Return 860.0\n",
            "Episode:  125 , Return 340.0\n",
            "Episode:  126 , Return 170.0\n",
            "Episode:  127 , Return 250.0\n",
            "Episode:  128 , Return 150.0\n",
            "Episode:  129 , Return 330.0\n",
            "Episode:  130 , Return 160.0\n",
            "Episode:  131 , Return 130.0\n",
            "Episode:  132 , Return 220.0\n",
            "Episode:  133 , Return 110.0\n",
            "Episode:  134 , Return 370.0\n",
            "Episode:  135 , Return 170.0\n",
            "Episode:  136 , Return 280.0\n",
            "Episode:  137 , Return 520.0\n",
            "Episode:  138 , Return 190.0\n",
            "Episode:  139 , Return 150.0\n",
            "Episode:  140 , Return 220.0\n",
            "Episode:  141 , Return 260.0\n",
            "Episode:  142 , Return 190.0\n",
            "Episode:  143 , Return 240.0\n",
            "Episode:  144 , Return 110.0\n",
            "Episode:  145 , Return 210.0\n",
            "Episode:  146 , Return 230.0\n",
            "Episode:  147 , Return 200.0\n",
            "Episode:  148 , Return 880.0\n",
            "Episode:  149 , Return 330.0\n",
            "Episode:  150 , Return 260.0\n",
            "Episode:  151 , Return 260.0\n",
            "Episode:  152 , Return 100.0\n",
            "Episode:  153 , Return 490.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-ecdbc095d513>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#env.render()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_greedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-a8250ca702a1>\u001b[0m in \u001b[0;36mepsilon_greedy\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mQ_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1606\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1608\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                **kwargs):\n\u001b[1;32m    262\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorLikeDataAdapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m     sample_weight_modes = broadcast_sample_weight_modes(\n\u001b[1;32m    265\u001b[0m         sample_weights, sample_weight_modes)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_process_tensorlike\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m   1014\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1016\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert_numpy_and_scipy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1017\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m   \u001b[0mflat_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m   \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mflat_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    653\u001b[0m                           expand_composites=expand_composites)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m   \u001b[0mflat_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m   \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mflat_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mflatten\u001b[0;34m(structure, expand_composites)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m   \u001b[0mexpand_composites\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_pywrap_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}