{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Actor Critic Methods - (A3C)",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMC2uq843d6mX5RC1p+iQyi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dude123studios/AdvancedReinforcementLearning/blob/main/Actor_Critic_Methods_(A3C).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qt8nqzkcU5lT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b889099e-4530-4b35-cafc-4afc01fa71f9"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import gym\n",
        "import multiprocessing\n",
        "import threading\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1etFXsn4XVad"
      },
      "source": [
        "env = gym.make('MountainCarContinuous-v0')\n",
        "state_shape = env.observation_space.shape[0]\n",
        "action_shape = env.action_space.shape[0]\n",
        "action_bound = [env.action_space.low, env.action_space.high]\n",
        "num_workers = multiprocessing.cpu_count()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVjjLAp5aRzo"
      },
      "source": [
        "num_episodes = 2000\n",
        "num_timesteps = 200\n",
        "global_net_scope = 'Global_Net'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s9o37sZaeOz"
      },
      "source": [
        "update_global = 10\n",
        "gamma = 0.9\n",
        "beta = 0.01"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpAI8DfdakIe"
      },
      "source": [
        "class ActorCritic(object):\n",
        "\n",
        "    def __init__(self, scope, sess, globalAC=None):\n",
        "        self.sess = sess\n",
        "        self.actor_optimizer = tf.train.RMSPropOptimizer(1e-4, name='RMSPropA')\n",
        "        self.critic_optimizer = tf.train.RMSPropOptimizer(1e-3, name='RMSPropC')\n",
        "\n",
        "        if scope == global_net_scope:\n",
        "            with tf.variable_scope(scope):\n",
        "\n",
        "                self.state = tf.placeholder(tf.float32, [None, state_shape], 'state')\n",
        "\n",
        "                self.actor_params, self.critic_params = self.build_network(scope)[-2:]\n",
        "\n",
        "        else:\n",
        "            with tf.variable_scope(scope):\n",
        "\n",
        "                self.state = tf.placeholder(tf.float32, [None, state_shape], 'state')\n",
        "                self.action_dist = tf.placeholder(tf.float32, [None, action_shape], 'action')\n",
        "                self.target_value = tf.placeholder(tf.float32, [None, 1], 'Vtarget')\n",
        "\n",
        "                mean, variance, self.value, self.actor_params, self.critic_params = self.build_network(scope)\n",
        "\n",
        "                td_error = tf.subtract(self.target_value, self.value, name='TD_error')\n",
        "\n",
        "                with tf.name_scope('critic_loss'):\n",
        "                    \n",
        "                    self.critic_loss = tf.reduce_mean(tf.square(td_error))\n",
        "                \n",
        "                normal_dist = tf.distributions.Normal(mean, variance)\n",
        "\n",
        "                with tf.name_scope('actor_loss'):\n",
        "\n",
        "                    log_prob = normal_dist.log_prob(self.action_dist)\n",
        "                    entropy_pi = normal_dist.entropy()\n",
        "\n",
        "                    self.loss = log_prob * td_error + (beta * entropy_pi)\n",
        "\n",
        "                    self.actor_loss = tf.reduce_mean(-self.loss)\n",
        "                \n",
        "                with tf.name_scope('select_action'):\n",
        "\n",
        "                    self.action = tf.clip_by_value(tf.squeeze(normal_dist.sample(1), axis=0), action_bound[0], action_bound[1])\n",
        "                \n",
        "                with tf.name_scope('local_grad'):\n",
        "                    self.actor_grads = tf.gradients(self.actor_loss, self.actor_params)\n",
        "                    self.critic_grads = tf.gradients(self.critic_loss, self.critic_params)\n",
        "\n",
        "                with tf.name_scope('push'):\n",
        "                    self.update_actor_params = self.actor_optimizer.apply_gradients(zip(self.actor_grads, globalAC.actor_params))\n",
        "                    self.update_critic_params = self.critic_optimizer.apply_gradients(zip(self.critic_grads, globalAC.critic_params))\n",
        "\n",
        "                with tf.name_scope('pull'):\n",
        "                    self.pull_actor_params = [l_p.assign(g_p) for l_p, g_p in zip(self.actor_params, globalAC.actor_params)]\n",
        "                    self.pull_critic_params = [l_p.assign(g_p) for l_p, g_p in zip(self.critic_params, globalAC.critic_params)]\n",
        "\n",
        "    def build_network(self, scope):\n",
        "    \n",
        "        w_init = tf.random_normal_initializer(0., 1.)\n",
        "\n",
        "        with tf.variable_scope('actor'):\n",
        "\n",
        "            l_a = tf.layers.dense(self.state, 200, tf.nn.relu, kernel_initializer=w_init, name='la')\n",
        "            mean = tf.layers.dense(l_a, action_shape, tf.nn.tanh, kernel_initializer=w_init, name='mean')\n",
        "            variance = tf.layers.dense(l_a, action_shape, tf.nn.softplus, kernel_initializer=w_init, name='value')\n",
        "\n",
        "        with tf.variable_scope('critic'):\n",
        "\n",
        "            l_c = tf.layers.dense(self.state, 100, tf.nn.relu, kernel_initializer=w_init)\n",
        "            value = tf.layers.dense(l_c, 1, kernel_initializer=w_init, name='lc')\n",
        "\n",
        "        actor_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/actor')\n",
        "        critic_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/critic')\n",
        "\n",
        "        return mean, variance, value, actor_params, critic_params\n",
        "    \n",
        "    def update_global(self, feed_dict):\n",
        "        self.sess.run([self.update_actor_params, self.update_critic_params], feed_dict)\n",
        "    \n",
        "    def pull_from_global(self):\n",
        "        self.sess.run([self.pull_actor_params, self.pull_critic_params])\n",
        "\n",
        "    def select_action(self, state):\n",
        "\n",
        "        state = state[np.newaxis, :]\n",
        "        return self.sess.run(self.action, {self.state: state})[0]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zBIKy25nWiR"
      },
      "source": [
        "class Worker(object):\n",
        "\n",
        "    def __init__(self, name, globalAC, sess):\n",
        "\n",
        "        self.env = gym.make('MountainCarContinuous-v0').unwrapped\n",
        "\n",
        "        self.name = name\n",
        "\n",
        "        self.AC = ActorCritic(name, sess, globalAC)\n",
        "\n",
        "        self.sess = sess\n",
        "\n",
        "    \n",
        "    def work(self):\n",
        "        global global_rewards, global_episodes\n",
        "\n",
        "        total_step = 1\n",
        "        batch_states, batch_actions, batch_rewards = [], [], []\n",
        "\n",
        "        while not coord.should_stop() and global_episodes < num_episodes:\n",
        "\n",
        "            state = self.env.reset()\n",
        "\n",
        "            Return = 0\n",
        "\n",
        "            for t in range(num_timesteps):\n",
        "\n",
        "                action = self.AC.select_action(state)\n",
        "                \n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "\n",
        "                done = (t == num_timesteps-1) \n",
        "\n",
        "                Return += reward \n",
        "                \n",
        "                batch_states.append(state)\n",
        "                batch_actions.append(action)\n",
        "                batch_rewards.append((reward+8)/8)\n",
        "\n",
        "                if total_step % update_global == 0 or done:\n",
        "                    if done:\n",
        "                        v_s_ = 0\n",
        "                        \n",
        "                    else:\n",
        "                        v_s_ = self.sess.run(self.AC.value, {self.AC.state: next_state[np.newaxis, :]})[0, 0]\n",
        "                        \n",
        "                    batch_target_value = []\n",
        "\n",
        "                    for reward in batch_rewards[:: -1]:\n",
        "                        v_s_ = reward + gamma * v_s_\n",
        "                        batch_target_value.append(v_s_)\n",
        "\n",
        "                    batch_target_value.reverse()\n",
        "\n",
        "                    batch_states, batch_actions, batch_target_value = (\n",
        "                        np.vstack(batch_states), np.vstack(batch_actions), \n",
        "                        np.vstack(batch_target_value)\n",
        "                    )\n",
        "\n",
        "                    feed_dict = {\n",
        "                        self.AC.state: batch_states,\n",
        "                        self.AC.action_dist: batch_actions,\n",
        "                        self.AC.target_value: batch_target_value\n",
        "                    }\n",
        "\n",
        "                    self.AC.update_global(feed_dict)\n",
        "\n",
        "                    batch_states, batch_actions, batch_rewards = [], [], []\n",
        "\n",
        "                    self.AC.pull_from_global()\n",
        "                    \n",
        "                state = next_state\n",
        "                total_step += 1\n",
        "\n",
        "                if done:\n",
        "                    global_rewards.append(Return)\n",
        "                    if len(global_rewards) > 5:\n",
        "                        global_rewards[-1] = (np.mean(global_rewards[-5:]))\n",
        "                        \n",
        "                    global_episodes += 1\n",
        "                    break\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JH3dS5NtQFb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33da10cd-3ccc-46ac-8112-c51d0bda6554"
      },
      "source": [
        "global_rewards = []\n",
        "global_episodes = 0\n",
        "\n",
        "sess = tf.Session()\n",
        "\n",
        "with tf.device(\"/cpu:0\"):\n",
        "\n",
        "    global_agent = ActorCritic(global_net_scope, sess)\n",
        "\n",
        "    worker_agents = []\n",
        "\n",
        "    for i in range(num_workers):\n",
        "        i_name = 'W_%i' % i \n",
        "        worker_agents.append(Worker(i_name, global_agent, sess))\n",
        "\n",
        "coord = tf.train.Coordinator()\n",
        "\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "worker_threads = []\n",
        "\n",
        "for worker in worker_agents:\n",
        "    job = lambda: worker.work()\n",
        "    t = threading.Thread(target=job)\n",
        "    t.start()\n",
        "    worker_threads.append(t)\n",
        "\n",
        "coord.join(worker_threads)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-5-166a9299c150>:30: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/distributions/normal.py:160: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/rmsprop.py:123: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}